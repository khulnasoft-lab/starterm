//! Logic for generating executable workflows from natural language prompts.
//! This is a key component of the "Agent Mode Eval" feature.

use super::context::AiContext;
use crate::workflows::Workflow;
use serde::{Deserialize, Serialize};
use crate::ai::llm::client::{LlmClient, ChatMessage, LlmRequest, LlmError};
use std::sync::Arc;

/// A workflow generated by the AI, including an explanation.
#[derive(Debug, Serialize, Deserialize)]
pub struct GeneratedWorkflow {
    pub workflow: Workflow,
    pub explanation: String,
}

/// Calls an LLM to convert a prompt into a `GeneratedWorkflow`.
///
/// This function constructs a detailed request to the LLM, providing the
/// prompt, the context, and examples of the desired JSON output format.
pub async fn generate_workflow_from_prompt(
    prompt: &str,
    context: &AiContext<'_>,
    llm_client: Arc<dyn LlmClient>, // Use the generic trait
) -> Result<GeneratedWorkflow, String> {

    // 1. Construct the detailed prompt for the LLM.
    let system_prompt = format!(
        "You are an expert command-line assistant for the {} shell. \
        The user will provide a task. Your goal is to generate a JSON object \
        that represents a 'Workflow'. The JSON must conform to this structure: \
        {{ \"workflow\": {{ \"name\": \"...\", \"description\": \"...\", \"steps\": [...] }}, \
        \"explanation\": \"...\" }}. The 'steps' array can contain items of type \
        `{{\"Execute\":\"command to run\"}}` or `{{\"Ask\":{{...}}`. \
        Analyze the user's request and the provided terminal context to create the most \
        effective and safe series of commands.",
        context.shell_type
    );

    let user_prompt = format!(
        "Task: \"{}\"\n\n\
        Current Context:\n- CWD: {}\n- Scrollback (last 3 lines):\n{}\n\n\
        Generate the JSON for the workflow.",
        prompt,
        context.cwd,
        context.scrollback.join("\n")
    );

    // 2. Create the request for the LLM client.
    let request = LlmRequest {
        model: "gpt-4o".to_string(), // Or get from config
        messages: vec![
            ChatMessage { role: "system".to_string(), content: system_prompt },
            ChatMessage { role: "user".to_string(), content: user_prompt },
        ],
    };

    // 3. Execute the request using the provided client.
    match llm_client.execute_chat(request).await {
        Ok(response) => {
            // 4. Deserialize the JSON response from the LLM's content.
            serde_json::from_str(&response.content)
                .map_err(|e| format!("LLM returned invalid JSON: {}", e))
        }
        Err(e) => Err(format!("Failed to communicate with LLM: {}", e)),
    }
} 