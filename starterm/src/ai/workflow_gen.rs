//! Logic for generating executable workflows from natural language prompts.
//! This is a key component of the "Agent Mode Eval" feature.

use super::context::AiContext;
use crate::workflows::Workflow;
use serde::{Deserialize, Serialize};

/// A workflow generated by the AI, including an explanation.
#[derive(Debug, Serialize, Deserialize)]
pub struct GeneratedWorkflow {
    pub workflow: Workflow,
    pub explanation: String,
}

/// Calls an external LLM API (or a local model) to convert a prompt into a workflow.
///
/// This function would construct a detailed request to the LLM, providing the
/// prompt, the context, and examples of the desired JSON output format.
pub async fn generate_workflow_from_prompt(
    prompt: &str,
    _context: &AiContext<'_>,
) -> Result<GeneratedWorkflow, String> {
    // TODO: Implement the actual LLM API call using `reqwest`.
    // 1. Construct a detailed prompt for the LLM, including the user's request,
    //    the AiContext, and instructions to format the output as JSON
    //    matching the `GeneratedWorkflow` struct.
    // 2. Make an asynchronous HTTP POST request to the LLM endpoint (e.g., OpenAI, Anthropic).
    // 3. Deserialize the JSON response back into a `GeneratedWorkflow` struct.

    // Placeholder implementation:
    if prompt.contains("list files and count them") {
        let workflow = Workflow::new("Generated: List and Count", "ls -l && ls -l | wc -l")
            .with_description("Lists files and then counts them.".to_string());
        Ok(GeneratedWorkflow {
            workflow,
            explanation: "This workflow first lists the files in long format, then pipes the output to `wc -l` to count the number of lines.".to_string(),
        })
    } else {
        Err("The AI model could not understand the request. (Placeholder response)".to_string())
    }
} 